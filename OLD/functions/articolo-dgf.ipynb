{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four different models have been compared for fitting SARS-nCoV-2 total cumulative cases curves in 185 countries over a period of 97 days. Evaluated models have been: _Simple Logistic Function_ (**SLF**), _Simple Gompertz Function_ (**SGF**), _Double Logistic Function_ (**DLF**) and _Double Gompertz Function_ (**DGF**). \n",
    "**DGF** model showed lower MSE, RMSE, NRMSE, MAE, NAE and higher Pearson R compared to the others. \n",
    "\n",
    "**SGF** $\\chi^2$ scores have been found higher in:\n",
    "170 countries compared with **SLF** ($\\mu=0.989$, $\\sigma=0.050$);\n",
    "174 countries compared with **SGF** ($\\mu=0.969$, $\\sigma=0.110$);\n",
    "154 countries compared with **DLF** ($\\mu=0.992$, $\\sigma=0.046$).\n",
    "\n",
    "**SGF** $\\chi^2_\\nu$ scores have been found higher:\n",
    "165 countries compared with **SLF** ($\\mu=0.902$, $\\sigma=0.176$);\n",
    "155 countries compared with **SGF** ($\\mu=0.889$, $\\sigma=0.185$);\n",
    "143 countries compared with **DLF** ($\\mu=0.899$, $\\sigma=0.180$).\n",
    "\n",
    "**SGF** AIC scores have been found higher:\n",
    "158 countries compared with **SLF** ($\\mu=0.985$, $\\sigma=0.068$);\n",
    "153 countries compared with **SGF** ($\\mu=0.991$, $\\sigma=0.049$);\n",
    "137 countries compared with **DLF** ($\\mu=0.991$, $\\sigma=0.044$).\n",
    "\n",
    "**SGF** BIC scores have been found higher in:\n",
    "149 countries compared with **SLF** ($\\mu=0.985$, $\\sigma=0.063$);\n",
    "143 countries compared with **SGF** ($\\mu=0.990$, $\\sigma=0.046$);\n",
    "137 countries compared with **DLF** ($\\mu=0.992$, $\\sigma=0.044$).\n",
    "\n",
    "Results suggest that _Double Gompertz Function_ may be a good fitting model for SARS-nCoV-2 analysis and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARS-nCoV-2 total cumulative cases data have been gathered from Johns Hopkins University GitHub repository [REF] and summed into single countries where regional level was provided [REF]. Data have been used \"as is\" without rejecting any outlier and/or error (negative daily $\\Delta$).\n",
    "Data and results have been stored in a `pandas` n-dimensional `DataFrame`.\n",
    "\n",
    "Raw data contained 185 countries and daily cumulative confirmed cases for 97 days, from 2020-01-22 to 2020-04-29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have been defined with `lmfit` (implementation of classical `curve_fit` in `scipy`) using Nelder-Mead method for fitting [REF].\n",
    "\n",
    "Total residual from each function have been initially compared (unsorted, sorted, gaussian distribution) to find the model with residual $\\mu$ closer to $0$ and shorter $\\sigma$. _Akaike Information Criterion_ (**AIC**) mean,  _Bayesian Information Criterion_ (**BIC**) mean and $\\chi^2$ scores mean have been used to find the likely better fitting model that has been finally compared, country by country, with _AIC scores_ probability (**AICp**), _BIC scores_ probability (**BICp**) and $\\chi^2$ scores probability in relative probability distribuition space. See Appendix [REF] for formulas.\n",
    "\n",
    "Models have been defined as follow:\n",
    "\n",
    "- Simple Logisic Function (**SLF**):\n",
    "\n",
    "```python\n",
    "def logit_function(x, a, b, k, e):\n",
    "    d = k * (b - np.array(x))\n",
    "    return (a / (1 + np.exp(d))) + e\n",
    "\n",
    "```\n",
    "$$ f(t) = \\frac{ a }{ 1 + e^{ k (b - t) } } + \\varepsilon $$\n",
    "\n",
    "- Double Logisic Function (**DLF**):\n",
    "\n",
    "```python\n",
    "def double_logit_function(x, a1, b1, k1, a2, b2, k2, e):\n",
    "    d1 = k1 * (b1 - np.array(x))\n",
    "    g1 = a1 / (1 + np.exp(d1))\n",
    "    d2 = k2 * (b2 - np.array(x))\n",
    "    g2 = (a2 - a1) / (1 + np.exp(d2))\n",
    "    return g1 + g2 + e\n",
    "```\n",
    "$$ f(t) = \\frac{ a_1 }{ 1 + e^{ k_1 (b_1 - t) } } + \\frac{ a_2 - a_1 }{ 1 + e^{ k_2 (b_2 - t) } } + \\varepsilon $$\n",
    "\n",
    "- Simple Gompertz Function (**SGF**):\n",
    "\n",
    "```python\n",
    "def gompertz_function(x, a, b, k, e):\n",
    "    exp = - np.exp(k * (b - x))\n",
    "    return a * np.exp(exp) + e\n",
    "```\n",
    "$$ f(t) = a \\cdot e^{ -e^{ k (b - t) } } + \\varepsilon $$\n",
    "\n",
    "- Double Gompertz Function (**DGF**):\n",
    "\n",
    "```python\n",
    "def double_gompertz_function(x, a1, b1, k1, a2, b2, k2, e):\n",
    "    exp1 = - np.exp(k1 * (b1 - x))\n",
    "    g1 = a1 * np.exp(exp1)\n",
    "    exp2 = - np.exp(k2 * (b2 - x))\n",
    "    g2 = (a2 - a1) * np.exp(exp2)\n",
    "    return g1 + g2 + e\n",
    "```\n",
    "$$ f(t) = a_1 \\cdot e^{ -e^{ k_1 (b_1 - t) } } + (a_2 - a_1) \\cdot e^{ -e^{ k_2 (b_2 - t) } } + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting has been performed with `lmfit` using Nelder-Mead method\n",
    "\n",
    "```python\n",
    "model = lmfit.Model(function)\n",
    "result = model.fit(data=y, params=p, x=x, method='Nelder', nan_policy='omit')\n",
    "```\n",
    "\n",
    "initial parameters `p` have been guessed as follows (where $\\hat{y}$ are measured values):\n",
    "\n",
    "- **SLF** and **SGF**\n",
    "\n",
    "```python\n",
    "p = model.make_params(\n",
    "    a=y[-1],\n",
    "    b=max_y_i,\n",
    "    k=.1,\n",
    "    e=y[0]\n",
    ")\n",
    "```\n",
    "\n",
    "$$ a = \\hat{y}_{-1} $$\n",
    "$$ b = x_{\\mathbf{max}(d\\hat{y})} $$\n",
    "$$ k = 0.1 $$\n",
    "$$ \\varepsilon = \\hat{y}_0 $$\n",
    "\n",
    "- **DLF** and **DGF**\n",
    "\n",
    "```python\n",
    "p = model.make_params(\n",
    "    a1=y[max_y_i] * 2,\n",
    "    b1=max_y_i,\n",
    "    k1=.1,\n",
    "    a2=max(y),\n",
    "    b2=len(y),\n",
    "    k2=.1,\n",
    "    e=y[0]\n",
    ")\n",
    "```\n",
    "\n",
    "$$ a_1 = 2 \\hat{y}_{\\mathbf{max}(d\\hat{y})} $$\n",
    "$$ b_1 = x_{\\mathbf{max}(d\\hat{y})} $$\n",
    "$$ k_1 = 0.1 $$\n",
    "$$ a_2 = \\mathbf{max}(\\hat{y}) $$\n",
    "$$ b_2 = x_{\\hat{y}_{-1}} $$\n",
    "$$ k_2 = 0.1 $$\n",
    "$$ \\varepsilon = \\hat{y}_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting failed for one country only (Yemen) returning best fit information from 184 countries, for a total of 17,848 points for each of the four models.\n",
    "\n",
    "Complete `python` backend for data gathering, fitting and analysis along with a `pickle` saved dataframe of all measured data and results is online avalaible at [REF]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting examples are reported in figures [REF] [REF] [REF]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several skill score have been used to evaluate to average skil and skill interpolating extreme values (outliers).\n",
    "\n",
    "Mean absolute error (**MAE**) is a natural, unambiguous, measure of average error [Willmott and Matsuura,\n",
    "2006]. It shows the errors in the same unit as variables themselves. MAE is bounded below by 0 (best case) and unbounded above. Disadvatage is that, taking absolute error values, positive and negative errors can cancel out.\n",
    "\n",
    "Normalized Absolute Error (**NAE**) ...\n",
    "\n",
    "Mean Squared Error (**MSE**) ...\n",
    "\n",
    "Root Mean Squared Error (**RMSE**) is very commonly used as a measure of deviation from the observed value. Although is has been criticized as being ambiguous [Willmott and Matsuura, 2006] and its dependence on the squared error means that it is not resistant to outliers deviating from a Gaussian distribution. It has been included because of its sensitivity to large outliers.\n",
    "\n",
    "Normalized Root Mean Squared Error (**NRMSE**) ...\n",
    "\n",
    "Pearson Correlation coefficient (**Pearson R**) depends on squared deviations and so is not a resistant measure. However, this statistic removes the effect of any bias in the interpolated data. Problems with correctly capturing the variance will not be highlighted as the measure normalizes the observed and modeled values by their standard deviations. The statistic is standardized. However, because of its insensitivity to biases and errors in variance, the correlation coefficient should be considered as a measure of potential skill [Murphy and Epstein, 1989; Wilks, 2006].\n",
    "\n",
    "Chi-Square (**$\\chi^2$** ) and Chi-Square score (or weight) and its relative probability distribution space ...\n",
    "Within the probability distribution space, bounded from $-1$ to $1$, a $p$ value greater than $0.5$ means model $H_0$ has more chances to be better fitting than alternative model $H_1$.\n",
    "\n",
    "Reduced Chi-Square (**$\\chi^2_\\nu$** ) and Reduced Chi-Square score (or weight) and its relative probability distribution space ...\n",
    "Within the probability distribution space, bounded from $-1$ to $1$, a $p$ value greater than $0.5$ means model $H_0$ has more chances to be better fitting than alternative model $H_1$.\n",
    "\n",
    "Akaike Information Criterion (**AIC**), its score (or weight) and its relative probability distribution space ...\n",
    "Within the probability distribution space, bounded from $-1$ to $1$, a $p$ value greater than $0.5$ means model $H_0$ has more chances to be better fitting than alternative model $H_1$.\n",
    "\n",
    "Bayesian Information Criterion (**BIC**), its score (or weight) and its relative probability distribution space ...\n",
    "Within the probability distribution space, bounded from $-1$ to $1$, a $p$ value greater than $0.5$ means model $H_0$ has more chances to be better fitting than alternative model $H_1$.\n",
    "\n",
    "Relative probability distribution spaces have been used to evaluate information criteria differences ($\\Delta I$).\n",
    "\n",
    "$$\n",
    "p = \\frac{ e^{ -\\frac{1}{2} (I_1 - I_0)} }{ 1 + e^{ -\\frac{1}{2} (I_1 - I_0)} }\n",
    "$$\n",
    "\n",
    "where $I_i$ is information criterion of model $i$ and $I_0 < I_1$. A $p < 0.5$ show a higher probabilty of null hypothesis model $H_0$. A $p = 0.5$ indicates a $50\\%$ for both hypothesis. A $p > 0.5$ suggets the probability to reject null hypothesi in favour to alternative hypothesis model $H_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](relativep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta I$ criteria relative probabilty densities have been computed and plotted to summarize null hypothesis model $H_0$ tests against alternative hypothesis models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total residual from each model have been collected and compared to get a first \"rough\" evidence of the most likely better fitting model [FIG]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](residualstats3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SLF** showed a residual mean $\\mu=1.694$ closer to $0$ but the wider standard deviation (RMSE) $\\sigma=1247$. **DGF** showed the lower residual standard deviation $\\sigma=351$ and a mean $\\mu=-5.197$.\n",
    "\n",
    "Mean Squared Error (**MSE**, Variance), Root Mean Squared Error (**RMSE**, Standard Deviation), Normalize Root Mean Squared Error (**NRMSE**, Normalized Standard Deviation), Mean Absolute Error (**MAE**), Normalized Absolute Error (**NAE**) and Pearson Correlation (**Pearson R**) have been computed for all models residual [FIG] (see Appendix for formulas [REF]). **DGF** showed the best results for all indices confirming the first null hypothesis that could have been the best fitting model among the chosen ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](residualstatsok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since compared models are nested (**SLF** and **SGF** have 4 parameters; **DLF** and **DGF** have 7 parameters) Chi-Squared (**$\\chi^2$**), Reduced Chi Square (**$\\chi^2\\nu$**), Akaike Information Criterion (**AIC**) and Bayesian Information Criterion (**BIC**) scores have been used instead of classical $H_0$ null hypothesis. **$\\chi^2$**s, **$\\chi^2\\nu$**, **AIC**s and **BIC**s have been collected from all fits and compared with each other in relative probability distribution space, country by country, using values returned by `lmfit.minimize` [REF].\n",
    "\n",
    "Information Criteria $\\Delta$s have been evaluated in relative probabilty space and gathered in density plots (10 bins) [FIG]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](criteriastats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](chiscoresok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](redchiscoresok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](aicscoresok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](bicscoresok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcuting mean $\\mu$ and standard deviation $\\sigma$ for $\\chi^2$, $\\chi^2_\\nu$, **AIC** and **BIC** scores [ADD DIFFERENCE] strongly confirmed _Double Gompertz Function_ as the better fitting model for SARS-nCoV-2 cumulative cases curve fitting. Results also showed that **DGF** is not only much more fitting than models with less parameters (**SLF** and **SGF**) as expected but also compared to _Double Logistic Function_ with the same degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the compared models _Double Gompertz Function_ has showed the best results and scores fitting data of SARS-nCoV-2 cumulative cases, suggesting that this model should be studied more deeply (possibly improved) and compared to other existing models for further analysis, including forecasting capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![img](fit1.png)\n",
       "\n",
       "***\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![img](fit2.png)\n",
       "\n",
       "***\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![img](fit3.png)\n",
       "\n",
       "***\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![img](fit4.png)\n",
       "\n",
       "***\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![img](fit5.png)\n",
       "\n",
       "***\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![img](fit6.png)\n",
       "\n",
       "***\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for j in range(6):\n",
    "    display(Markdown(f\"![img](fit{j+1}.png)\\n\\n***\\n\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all formulas we assume: $\\hat{y}$ as the measured (true) value and $y$ as the predicted value; $n$ is the number of values and $n_{var}$ the number of model's variable parameters.\n",
    "\n",
    "***\n",
    "\n",
    "- **MAE**: Mean absolute error (aka Mean Deviation)\n",
    "\n",
    "$$ \\mathbf{MAE} = \\frac{ \\sum \\left| y - \\hat{y} \\right| }{ n }$$\n",
    "\n",
    "- **NAE**: Normalized absolute error (aka Normalized Mean Deviation)\n",
    "\n",
    "$$ \\mathbf{NAE} = \n",
    "\\frac{\n",
    "    \\sum \\left| y - \\hat{y} \\right|\n",
    "}{\n",
    "    \\sum{\\hat{y}}\n",
    "}\n",
    "$$\n",
    "\n",
    "- **MSE**: Mean Squared Error (aka Variance)\n",
    "\n",
    "$$ \\mathbf{MSE} = \\sigma^2 = \\frac{1}{n} \\sum (y - \\hat{y})^2 $$\n",
    "\n",
    "- **RMSE**: Root Mean Squared Error (aka Standard Deviation)\n",
    "\n",
    "$$ \\mathbf{RMSE} = \\sigma = \\sqrt{ \\frac{1}{n} \\sum \\left( y - \\hat{y} \\right)^2 } $$\n",
    "\n",
    "- **NRMSE**: Normalizer Root Mean Squared Error (aka Normalized Standard Deviation)\n",
    "\n",
    "$$ \\mathbf{NRMSE} = \\sigma_\\nu = \n",
    "\\frac{\n",
    "    \\sqrt{ \\frac{1}{n} \\sum \\left( y - \\hat{y} \\right)^2 }\n",
    "}{\n",
    "    \\sum{\\hat{y}}\n",
    "}\n",
    "$$\n",
    "\n",
    "- **Pearson R**: Pearson Correlation Coefficient\n",
    "\n",
    "$$\n",
    "\\mathbf{R} = \\frac{\n",
    "    \\sum y \\hat{y} - \\frac{1}{n} \\sum y \\sum \\hat{y}\n",
    "    }{\n",
    "    \\sqrt{ \\sum y^2 - \\frac{1}{n} \\left(\\sum y\\right)^2 }\n",
    "    \\sqrt{ \\sum \\hat{y}^2 - \\frac{1}{n} \\left(\\sum \\hat{y} \\right)^2 }\n",
    "    }\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "- $\\chi^2$: Chi-Square\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\left( y - \\hat{y} \\right)^2\n",
    "$$\n",
    "\n",
    "- $\\chi^2$ **p**: Chi-Square score (or weight) in $\\chi^2$ relative probability distribution space:\n",
    "\n",
    "$$ \\mathbf{\\chi^2_p} =\n",
    "\\frac{\n",
    "    e^{\n",
    "        -0.5 \\cdot (\\mathbf{\\chi^2_1} - \\mathbf{\\chi^2_0})\n",
    "    }\n",
    "}{\n",
    "    1 + e^{\n",
    "        -0.5 \\cdot (\\mathbf{\\chi^2_1} - \\mathbf{\\chi^2_0})\n",
    "    }\n",
    "} $$\n",
    "\n",
    "$$\\mathbf{\\chi^2_0} \\leq \\mathbf{\\chi^2_1}$$\n",
    "\n",
    "***\n",
    "\n",
    "- $\\chi^2_\\nu$: Reduced Chi-Square\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\frac{ \\chi^2 }{ n - n_{var} }\n",
    "$$\n",
    "\n",
    "- $\\chi^2_\\nu$ **p**: Chi-Square score (or weight) in $\\chi^2$ relative probability distribution space:\n",
    "\n",
    "$$ \\mathbf{\\chi^2_{\\nu p}} =\n",
    "\\frac{\n",
    "    e^{\n",
    "        -0.5 \\cdot (\\mathbf{\\chi^2_{\\nu 1}} - \\mathbf{\\chi^2_{\\nu 0}})\n",
    "    }\n",
    "}{\n",
    "    1 + e^{\n",
    "        -0.5 \\cdot (\\mathbf{\\chi^2_{\\nu 1}} - \\mathbf{\\chi^2_{\\nu 0}})\n",
    "    }\n",
    "} $$\n",
    "\n",
    "$$\\mathbf{\\chi^2_{\\nu 0}} \\leq \\mathbf{\\chi^2_{\\nu 1}}$$\n",
    "\n",
    "***\n",
    "\n",
    "- **AIC**: Aikake Information Criterion\n",
    "\n",
    "$$\n",
    "\\mathbf{AIC} = n \\log{ \\left( \\frac{\\chi^2}{n} \\right) } + 2 n_{var}\n",
    "$$\n",
    "\n",
    "- **AIC p**: Aikake Information Criterion score (or weight) in **AIC** relative probability distribution space:\n",
    "\n",
    "$$ \\mathbf{AIC_p} =\n",
    "\\frac{\n",
    "    e^{\n",
    "        -0.5 \\cdot (\\mathbf{AIC_1} - \\mathbf{AIC_0})\n",
    "    }\n",
    "}{\n",
    "    1 + e^{\n",
    "        -0.5 \\cdot (\\mathbf{AIC_1} - \\mathbf{AIC_0})\n",
    "    }\n",
    "} $$\n",
    "\n",
    "$$\\mathbf{AIC_0} \\leq \\mathbf{AIC_1}$$\n",
    "\n",
    "***\n",
    "\n",
    "- **BIC**: Bayesian Information Criterion\n",
    "\n",
    "$$\n",
    "\\mathbf{BIC} = n \\log{ \\left( \\frac{\\chi^2}{n} \\right) } + \\log{\\left(n \\right)} n_{var}\n",
    "$$\n",
    "\n",
    "- **BIC p**: Bayesian Information Criterion score (or weight) in **AIC** relative probability distribution space:\n",
    "\n",
    "$$ \\mathbf{BIC_p} =\n",
    "\\frac{\n",
    "    e^{\n",
    "        -0.5 \\cdot (\\mathbf{BIC_1} - \\mathbf{BIC_0})\n",
    "    }\n",
    "}{\n",
    "    1 + e^{\n",
    "        -0.5 \\cdot (\\mathbf{BIC_1} - \\mathbf{BIC_0})\n",
    "    }\n",
    "} $$\n",
    "\n",
    "$$\\mathbf{BIC_0} \\leq \\mathbf{BIC_1}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
